# Big Data Development Standards - CODING-STANDARDS-BIG-DATA

**Version**: 2.0.0
**Scope**: Big data development roles (Batch processing/Stream processing/Data lake/Data warehouse, tech stack agnostic)
**Last Updated**: 2025-12-25

---

## ğŸš¨ Core Iron Laws (Inherited from common.md)

> **Must follow the four core iron laws from common.md**

```
Iron Law 1: SPEC is the Single Source of Truth (SSOT)
       - Data pipeline design must comply with SPEC definitions
       - Schema, data flow, processing logic based on SPEC

Iron Law 2: Intelligent Reuse and Destroy-and-Rebuild
       - Existing pipeline fully matches â†’ Reuse directly
       - Partial match â†’ Delete and rebuild

Iron Law 3: Prohibit Incremental Development
       - Prohibit adding new features to old pipelines
       - Prohibit retaining compatibility schemas

Iron Law 4: Context7 Research First
       - Use mature big data frameworks
       - Prohibit self-implementing ETL tools
```

---

## ğŸ—ï¸ Data Pipeline Design

### Pipeline Principles
- âœ… Single responsibility: Each job does only one thing
- âœ… Idempotency: Repeated execution produces same result
- âœ… Restartability: Support recovery from failure point
- âœ… Clear data lineage
- âŒ Avoid strong coupling between pipelines

### Data Flow Design
- âœ… Clear input, processing, output boundaries
- âœ… Use checkpoint mechanism
- âœ… Design data backfill strategy
- âœ… Distinguish incremental and full processing
- âœ… Handle late data
- âŒ Avoid circular dependencies

---

## ğŸ“Š Batch Processing Development

### Job Design
- âœ… Data partitioning (by time/region/business)
- âœ… Reasonable batch size settings
- âœ… Parallelism matches resources
- âœ… Failed tasks retryable
- âœ… Intermediate result persistence
- âŒ Avoid single point bottlenecks

### Scheduling Management
- âœ… Clear job dependencies (DAG)
- âœ… Set reasonable timeouts
- âœ… Configure alerts and monitoring
- âœ… Distinguish normal failures from abnormal failures
- âœ… Record job execution history
- âŒ Avoid hardcoded schedule times

### Data Quality
- âœ… Input data validation (schema, range, completeness)
- âœ… Data quality checks during processing
- âœ… Output data consistency validation
- âœ… Bad data isolation (bad data partitions)
- âœ… Data quality metrics monitoring

---

## âš¡ Stream Processing Development

### Stream Processing Principles
- âœ… Process unbounded data streams
- âœ… Event time vs processing time
- âœ… Windowing mechanisms (tumbling/sliding/session windows)
- âœ… Watermark handling late data
- âœ… State management and checkpoints
- âŒ Avoid unbounded state growth

### Real-Time Guarantees
- âœ… Clear latency requirements (second/minute level)
- âœ… Backpressure mechanism
- âœ… Flow control and rate limiting
- âœ… Monitor processing latency
- âŒ Avoid blocking operations

### Consistency Guarantees
- âœ… At Least Once vs Exactly Once
- âœ… Transactional output
- âœ… Deduplication mechanism
- âœ… Order guarantees (within partition)
- âœ… Idempotency design

---

## ğŸ—‚ï¸ Data Partitioning and Storage

### Partitioning Strategy
- âœ… Partition by time (year/month/day/hour)
- âœ… Partition by business dimension (region/category)
- âœ… Avoid data skew
- âœ… Partition pruning optimization
- âœ… Reasonable partition count control (< 10000)
- âŒ Avoid small file problem

### File Formats
- âœ… Use columnar storage (Parquet, ORC)
- âœ… Enable compression (Snappy, ZSTD)
- âœ… Schema evolution compatibility
- âœ… Reasonable file size (128MB-1GB)
- âŒ Avoid plain text formats (production environment)

### Data Lifecycle
- âœ… Define data retention policies
- âœ… Cold-hot data tiered storage
- âœ… Automatic historical data archiving
- âœ… Expired data cleanup
- âœ… Cost optimization storage

---

## ğŸ”„ Schema Management

### Schema Design
- âœ… Backward compatible schema evolution
- âœ… Use schema registry
- âœ… Versioned schema management
- âœ… Clear field types and constraints
- âŒ Avoid breaking changes

### Data Types
- âœ… Use appropriate data types (reduce storage and compute cost)
- âœ… Reasonable use of nested structures (avoid too deep)
- âœ… Timestamps uniformly use UTC
- âœ… String fields with length limits
- âŒ Avoid dynamic types (affects performance)

---

## âš™ï¸ Resource Management

### Resource Configuration
- âœ… Configure memory based on data volume
- âœ… Reasonable parallelism settings
- âœ… CPU and IO balance
- âœ… Resource isolation (tasks don't affect each other)
- âœ… Elastic scaling
- âŒ Avoid over-provisioning resources

### Performance Optimization
- âœ… Reduce data shuffle
- âœ… Use broadcast variables (small table joins)
- âœ… Local aggregation reduces network transfer
- âœ… Cache reused datasets
- âœ… Predicate pushdown
- âœ… Column pruning

### Cost Optimization
- âœ… Use spot instances (non-critical jobs)
- âœ… On-demand cluster start/stop
- âœ… Monitor resource utilization
- âœ… Optimize data storage costs
- âŒ Avoid idle resource waste

---

## ğŸ›¡ï¸ Data Security

### Access Control
- âœ… Least privilege principle
- âœ… Data classification management
- âœ… Sensitive data encrypted storage
- âœ… Audit logging
- âŒ Prohibit plaintext sensitive data storage

### Data Masking
- âœ… Production data masked for development testing
- âœ… Sensitive fields hashed or encrypted
- âœ… PII data (personal identity information) protection
- âœ… Data export permission control

---

## ğŸ“ˆ Monitoring and Observability

### Monitoring Metrics
- âœ… Job execution duration
- âœ… Data processing volume
- âœ… Resource utilization (CPU/memory/disk/network)
- âœ… Error rate and retry count
- âœ… Data latency (stream processing)
- âœ… Data quality metrics

### Alert Mechanisms
- âœ… Job failure alerts
- âœ… Data latency threshold alerts
- âœ… Data quality anomaly alerts
- âœ… Resource usage anomaly alerts
- âœ… SLA violation alerts

### Logging and Tracing
- âœ… Structured logging
- âœ… Log critical operations
- âœ… Distributed tracing (Trace ID)
- âœ… Data lineage tracking
- âŒ Avoid log flooding (over-logging)

---

## ğŸ§ª Testing

### Testing Strategy
- âœ… Unit tests (data transformation logic)
- âœ… Integration tests (end-to-end pipeline)
- âœ… Data quality tests
- âœ… Performance tests (large data volume)
- âœ… Boundary tests (empty data, bad data)

### Test Data
- âœ… Use production data samples
- âœ… Synthetic data generation
- âœ… Test environment data isolation
- âœ… Simulate data skew scenarios
- âŒ Prohibit testing in production environment

---

## ğŸ“‹ Big Data Development Checklist

- [ ] Pipeline idempotency and restartability
- [ ] Data partitioning reasonable (avoid small files and data skew)
- [ ] Schema version management and compatibility
- [ ] Resource configuration reasonable (memory, parallelism)
- [ ] Data quality validation
- [ ] Monitoring and alerting configured
- [ ] Sensitive data encryption and masking
- [ ] Logging and tracing complete
- [ ] Failure retry and fault tolerance
- [ ] Cost optimization (storage, compute)

---

---

## ğŸ›ï¸ Advanced Data Architecture (20+ years experience)

### Modern Data Architecture Paradigms
```
Data Mesh:
- Domain ownership: Data owned by domain teams
- Data as product: Data published as products
- Self-service platform: Unified infrastructure
- Federated governance: Decentralized governance
- Applicable: Large organizations, multi-domain

Data Lakehouse:
- Combine data lake and data warehouse advantages
- Delta Lake/Iceberg/Hudi table formats
- ACID transaction support
- Schema evolution and time travel
- Unified batch-stream processing

Lambda vs Kappa Architecture:
- Lambda: Batch + stream dual paths
- Kappa: Stream only, unified architecture
- Selection consideration: Complexity vs consistency
```

### Stream-Batch Unified Architecture
```
Unified Processing Engine:
- Apache Flink: Stream-batch unified
- Apache Beam: Cross-engine abstraction
- Spark Structured Streaming: Micro-batch + stream

Real-Time Data Warehouse:
- ODS (Operational Data Store): Real-time data lake ingestion
- DWD (Data Warehouse Detail): Real-time cleaning
- DWS (Data Warehouse Summary): Real-time aggregation
- ADS (Application Data Store): Real-time serving

Real-Time Features:
- Incremental Processing
- Materialized Views
- Change Data Capture (CDC)
```

### Data Governance Architecture
```
Metadata Management:
- Apache Atlas: Lineage tracking
- DataHub: Metadata platform
- Amundsen: Data discovery

Data Quality Frameworks:
- Great Expectations: Data validation
- Deequ: Spark data quality
- Data Contracts

Data Catalog:
- Automated discovery
- Business glossary
- Sensitive data classification
- Data asset search
```

---

## ğŸ”§ Essential Skills for Senior Big Data Experts

### Spark Deep Optimization
```
Memory Management:
- Executor Memory = Heap + Off-Heap
- spark.memory.fraction tuning
- Serialization (Kryo vs Java)
- Broadcast variable size control

Shuffle Optimization:
- spark.sql.shuffle.partitions tuning
- AQE (Adaptive Query Execution)
- Coalesce vs Repartition
- Skew handling (Salting)

Execution Plan Optimization:
- Predicate pushdown verification
- Join strategy selection (Broadcast/Sort-Merge/Shuffle-Hash)
- CBO (Cost-Based Optimizer)
- Catalyst optimizer understanding
```

### Flink Deep Optimization
```
State Management:
- State backend selection (Memory/RocksDB)
- Incremental checkpoints
- State TTL
- State size control

Backpressure Handling:
- Identify backpressure source
- Buffer tuning
- Parallelism adjustment
- Async IO

Exactly-Once Semantics:
- Two-phase commit (2PC)
- Idempotent writes
- Transactional Sink
- Changelog Stream
```

### Performance Tuning Methodology
```
Problem Diagnosis:
1. Confirm bottleneck (CPU/Memory/IO/Network)
2. Analyze execution plan
3. Identify data skew
4. Check resource configuration

Tuning Strategy:
- Data level: Partitioning, compression, format
- Operator level: Parallelism, memory, shuffle
- Cluster level: Resource allocation, queue configuration
- Code level: Avoid UDF, use vectorization
```

### Cost Optimization Practices
```
Storage Cost:
- Cold-hot tiering (S3 Glacier)
- Columnar compression (ZSTD/LZ4)
- Data lifecycle management
- Small file merging

Compute Cost:
- Spot/Preemptible instances
- Elastic scaling
- Resource utilization optimization
- Job scheduling optimization

FinOps Practices:
- Cost attribution (by team/project)
- Budget alerts
- Resource usage reports
- Continuous optimization loop
```

---

## ğŸš¨ Common Pitfalls for Senior Big Data Experts

### Architecture Traps
```
âŒ Excessive real-time:
- Use stream processing for all scenarios
- Increase complexity and cost
- Correct: Choose based on latency requirements

âŒ Ignore data quality:
- Only focus on pipeline functionality
- Dirty data pollutes downstream
- Correct: Data quality check gatekeeping

âŒ Schemaé‡è›® growth:
- Arbitrary field addition
- No version management
- Correct: Schema Registry, compatibility checks
```

### Performance Traps
```
âŒ Data skew not handled:
- Uneven join key distribution
- Single task drags overall
- Correct: Salting, Broadcast, AQE

âŒ Small file proliferation:
- High-frequency writes generate many small files
- High metadata pressure, slow queries
- Correct: Merge jobs, Compaction

âŒ Over-partitioning:
- Too many partitions
- Queries actually slower
- Correct: Reasonable partition granularity, < 10000
```

### Operational Traps
```
âŒ Unreliable checkpoints:
- Checkpoint failures not alerted
- Cannot recover on failure
- Correct: Checkpoint monitoring, backup verification

âŒ Resource over-provisioning:
- Every job configured with large memory
- Serious resource waste
- Correct: On-demand configuration, dynamic resource allocation

âŒ Ignore backfill scenarios:
- Only consider incremental processing
- Historical data cannot be processed
- Correct: Design backfill strategy
```

---

## ğŸ“Š Performance Monitoring Metrics

| Metric | Target | Alert Threshold | Measurement Tool |
|--------|--------|-----------------|------------------|
| Job Success Rate | > 99% | < 95% | Scheduling system |
| Data Latency (Stream) | < 1 minute | > 5 minutes | Monitoring system |
| Processing Throughput | SLA-based | < 80% expected | Metrics |
| Checkpoint Success Rate | 100% | < 99% | Flink Dashboard |
| Data Quality Pass Rate | > 99.9% | < 99% | Quality platform |
| Resource Utilization | 60-80% | < 30% or > 90% | Cluster monitoring |
| Shuffle Data Volume | Job-based | Abnormal growth | Spark UI |
| GC Time Percentage | < 5% | > 10% | JVM monitoring |
| Small File Count | < 1000/partition | > 5000 | Storage monitoring |
| Data Skew Ratio | < 2x | > 10x | Execution plan |

---

## ğŸ“‹ Big Data Development Checklist (Complete Version)

### Pipeline Design
- [ ] Idempotency and restartability
- [ ] Backfill strategy design
- [ ] Data lineage recording
- [ ] Late data handling

### Performance Optimization
- [ ] Partitioning strategy reasonable
- [ ] No data skew
- [ ] No small file issues
- [ ] Shuffle optimization

### Data Quality
- [ ] Input validation
- [ ] In-process checks
- [ ] Output validation
- [ ] Bad data isolation

### Operational Assurance
- [ ] Monitoring and alerting
- [ ] Logging and tracing
- [ ] Reliable checkpoints
- [ ] Cost optimization

---

**Big Data Development Principles Summary**:
Idempotency, Restartability, Data Quality, Partition Optimization, Resource Management, Monitoring and Alerting, Schema Evolution, Stream-Batch Unification, Cost Optimization, Data Security
