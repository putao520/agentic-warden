# Machine Learning Development Standards - CODING-STANDARDS-ML

**Version**: 2.0.0
**Scope**: Machine Learning development roles (Supervised/Unsupervised/Reinforcement Learning/Deep Learning, Framework Agnostic)
**Last Updated**: 2025-12-25

---

## ğŸš¨ Core Iron Rules (Inherited from common.md)

> **Must follow the four core iron rules from common.md**

```
Iron Rule 1: SPEC is the Single Source of Truth (SSOT)
       - Model requirements must comply with SPEC definitions
       - Metrics, data, deployment based on SPEC

Iron Rule 2: Smart Reuse and Destroy-Rebuild
       - Existing model fully matches â†’ Direct reuse
       - Partial match â†’ Retrain from scratch

Iron Rule 3: Prohibit Incremental Development
       - Prohibit fine-tuning old models for new features
       - Prohibit retaining compatibility features

Iron Rule 4: Context7 Research First
       - Use mature ML frameworks and tools
       - Prohibit implementing common algorithms yourself
```

---

## ğŸ“Š Data Management

### Data Quality
- âœ… Exploratory Data Analysis (EDA)
- âœ… Missing Value Detection and Handling
- âœ… Outlier Identification and Handling
- âœ… Data Consistency Verification
- âœ… Data Distribution Check
- âŒ Do Not Blindly Use Raw Data

### Data Preprocessing
- âœ… Standardized Data Cleaning Pipeline
- âœ… Feature Scaling (Normalization/Standardization)
- âœ… Categorical Encoding (One-Hot/Label/Target)
- âœ… Text Preprocessing (Tokenization/Stop Word Removal/Stemming)
- âœ… Image Preprocessing (Resize/Augmentation/Normalization)
- âŒ Avoid Data Leakage (Training Set Information Entering Test Set)

### Data Splitting
- âœ… Train/Validation/Test Set Separation
- âœ… Time Series Data Split by Time
- âœ… Stratified Sampling (Maintain Class Distribution)
- âœ… Cross-Validation (K-Fold/Stratified)
- âŒ Prohibit Using Test Set for Model Selection

---

## ğŸ”§ Feature Engineering

### Feature Construction
- âœ… Domain Knowledge-Driven Features
- âœ… Feature Interactions (Combined Features)
- âœ… Polynomial Features
- âœ… Time Feature Extraction (Day/Week/Month/Season)
- âœ… Aggregated Features (Statistics)
- âŒ Avoid Feature Explosion (Curse of Dimensionality)

### Feature Selection
- âœ… Correlation Analysis
- âœ… Feature Importance Ranking
- âœ… Recursive Feature Elimination (RFE)
- âœ… Regularization (L1/L2)
- âœ… Variance Threshold Filtering
- âŒ Avoid Retaining Redundant Features

### Feature Storage
- âœ… Feature Versioning
- âœ… Feature Reuse (Feature Store)
- âœ… Feature Documentation
- âœ… Automated Feature Pipeline
- âŒ Avoid Feature Inconsistency (Training vs Inference)

---

## ğŸ¤– Model Development

### Model Selection
- âœ… Baseline Model First (Simple Model)
- âœ… Select Model Type Based on Problem
- âœ… Compare Multiple Models
- âœ… Ensemble Learning
- âŒ Do Not Use Complex Models Too Early

### Training Process
- âœ… Set Random Seed (Reproducibility)
- âœ… Early Stopping
- âœ… Learning Rate Scheduling
- âœ… Gradient Clipping (Prevent Gradient Explosion)
- âœ… Reasonable Batch Size Settings
- âŒ Avoid Overfitting Training Set

### Hyperparameter Tuning
- âœ… Grid Search
- âœ… Random Search
- âœ… Bayesian Optimization
- âœ… Hyperparameter Importance Analysis
- âœ… Select Best Hyperparameters on Validation Set
- âŒ Do Not Tune on Test Set

---

## ğŸ“ˆ Model Evaluation

### Evaluation Metrics
- âœ… Select Metrics Based on Business Goals
- âœ… Classification: Accuracy/Precision/Recall/F1/AUC
- âœ… Regression: MAE/MSE/RMSE/RÂ²
- âœ… Ranking: NDCG/MAP
- âœ… Multi-Metric Comprehensive Evaluation
- âŒ Single Metric May Be Misleading

### Model Diagnosis
- âœ… Learning Curve Analysis
- âœ… Confusion Matrix (Classification)
- âœ… Residual Analysis (Regression)
- âœ… Feature Importance
- âœ… Error Sample Analysis
- âŒ Do Not Ignore Model Bias

### Overfitting Prevention
- âœ… Regularization (L1/L2/Dropout)
- âœ… Data Augmentation
- âœ… Early Stopping
- âœ… Cross-Validation
- âœ… Simplify Model Complexity
- âŒ Avoid Perfect Training Performance but Poor Generalization

---

## ğŸ”¬ Experiment Management

### Experiment Tracking
- âœ… Record Hyperparameter Configuration
- âœ… Record Training Metrics
- âœ… Record Model Version
- âœ… Record Data Version
- âœ… Use Experiment Tracking Tools (MLflow/Weights & Biases)
- âŒ Do Not Manually Record Experiments

### Reproducibility
- âœ… Fixed Random Seed
- âœ… Environment Configuration Versioning (requirements.txt)
- âœ… Code Version Control (Git)
- âœ… Data Version Control (DVC)
- âœ… Parameterized Training Scripts
- âŒ Avoid Unclear Environment Dependencies

---

## ğŸš€ Model Deployment

### Model Serialization
- âœ… Standard Format Saving (ONNX/SavedModel)
- âœ… Model Versioning
- âœ… Model Metadata Recording
- âœ… Model Compression (Quantization/Pruning)
- âŒ Avoid Using Incompatible Formats

### Inference Optimization
- âœ… Batch Inference
- âœ… Model Quantization (INT8/FP16)
- âœ… Model Distillation (Teacher-Student)
- âœ… Inference Acceleration (TensorRT/ONNX Runtime)
- âœ… Cache Hot Predictions
- âŒ Avoid Excessive Inference Latency

### Online Service
- âœ… Standardized API Interface
- âœ… Input Validation
- âœ… Timeout and Retry Mechanism
- âœ… Load Balancing
- âœ… A/B Testing
- âŒ Do Not Return Model Output Directly (Needs Post-Processing)

---

## ğŸ“Š Monitoring and Maintenance

### Performance Monitoring
- âœ… Monitor Inference Latency
- âœ… Monitor Prediction Accuracy
- âœ… Monitor Resource Usage (CPU/GPU/Memory)
- âœ… Monitor Throughput
- âŒ Do Not Deploy Without Monitoring

### Data Drift Detection
- âœ… Input Distribution Monitoring
- âœ… Feature Drift Detection
- âœ… Concept Drift Detection
- âœ… Automatic Alerting
- âœ… Trigger Model Retraining
- âŒ Avoid Model Staleness

### Model Updates
- âœ… Continuous Learning (Incremental Learning)
- âœ… Periodic Retraining
- âœ… New Version Canary Release
- âœ… Rollback Mechanism
- âŒ Do Not Interrupt Online Service

---

## ğŸ”’ Security and Privacy

### Data Privacy
- âœ… Data Desensitization
- âœ… Differential Privacy
- âœ… Federated Learning (Distributed Training)
- âœ… Access Control
- âŒ Prohibit Leaking Training Data

### Model Security
- âœ… Adversarial Sample Detection
- âœ… Model Watermarking
- âœ… Input Validation (Injection Prevention)
- âœ… Output Filtering
- âŒ Avoid Model Reverse Engineering

### Fairness
- âœ… Detect Model Bias
- âœ… Fairness Metrics Evaluation
- âœ… Data Balancing
- âœ… Bias Mitigation Techniques
- âŒ Do Not Ignore Ethical Issues

---

## ğŸ§ª Testing

### Unit Tests
- âœ… Data Preprocessing Function Tests
- âœ… Feature Engineering Function Tests
- âœ… Model Inference Function Tests
- âœ… Boundary Condition Tests
- âŒ Avoid Insufficient Test Coverage

### Integration Tests
- âœ… End-to-End Pipeline Tests
- âœ… Data Validation Tests
- âœ… Model Loading Tests
- âœ… API Interface Tests
- âŒ Do Not Skip Integration Tests

### Model Tests
- âœ… Invariance Tests (Input Changes, Output Unchanged)
- âœ… Directed Expectation Tests (Specific Input, Expected Output)
- âœ… Performance Benchmark Tests
- âœ… Adversarial Sample Tests
- âŒ Avoid Testing Only Happy Paths

---

## ğŸ“‹ Machine Learning Development Checklist

- [ ] Data quality validation and preprocessing
- [ ] Proper train/validation/test split
- [ ] Feature engineering and selection
- [ ] Baseline model establishment
- [ ] Hyperparameter tuning (validation set)
- [ ] Model evaluation (multiple metrics)
- [ ] Overfitting check
- [ ] Experiment reproducibility (seed/environment/version)
- [ ] Model deployment and inference optimization
- [ ] Monitoring and data drift detection

---

---

## ğŸ›ï¸ Advanced ML Architecture (20+ Years Experience)

### MLOps Maturity Model
```
Level 0 - Manual Process:
- Manual training and deployment
- No version control
- No monitoring

Level 1 - ML Pipeline Automation:
- Automated training pipeline
- Automated feature engineering
- Experiment tracking

Level 2 - CI/CD Pipeline:
- Model CI/CD
- Automated testing
- Automated deployment

Level 3 - Complete MLOps:
- Continuous Training (CT)
- Continuous Monitoring (CM)
- Automatic Retraining
- Feature Store
```

### Feature Store Architecture
```
Core Components:
- Feature Registry: Metadata, Lineage, Version
- Offline Storage: Batch Features (Data Lake)
- Online Storage: Real-Time Features (Redis/DynamoDB)
- Feature Service: Low-Latency API

Key Capabilities:
- Training-Inference Consistency
- Time Travel (Point-in-Time)
- Feature Reuse
- Data Quality Validation

Technology Selection:
- Feast: Open-Source, Lightweight
- Tecton: Enterprise-Grade
- SageMaker Feature Store: AWS Integration
```

### Large-Scale Training Architecture
```
Distributed Training:
- Data Parallelism
- Model Parallelism
- Pipeline Parallelism
- Hybrid Parallelism

Large Model Training:
- DeepSpeed ZeRO
- Megatron-LM
- FSDP (Fully Sharded Data Parallel)
- Gradient Checkpointing

Hardware Acceleration:
- GPU Clusters (NVIDIA A100/H100)
- TPU Pods (Google Cloud)
- Mixed Precision Training (AMP)
- Compilation Optimization (TorchScript/XLA)
```

### LLM/Generative AI Architecture
```
RAG (Retrieval-Augmented Generation):
- Vector Databases: Pinecone/Milvus/pgvector
- Embedding Models: OpenAI/Cohere/Local
- Retrieval Strategy: Semantic/Hybrid
- Generation Models: GPT/Claude/Open-Source

Fine-Tuning Strategy:
- Full Fine-Tuning
- LoRA/QLoRA (Parameter-Efficient)
- Prompt Tuning
- RLHF/DPO

Production Deployment:
- vLLM/TGI (Inference Optimization)
- Quantization (INT4/INT8/AWQ/GPTQ)
- Caching (KV Cache/Prompt Cache)
- Load Balancing
```

---

## ğŸ”§ Essential Skills for Senior ML Experts

### Deep Learning Tuning
```
Hyperparameter Strategy:
- Learning Rate: Warmup + Cosine Decay
- Batch Size: Progressive Increase
- Regularization: Dropout + Weight Decay
- Data Augmentation: Automation (AutoAugment)

Training Stability:
- Gradient Clipping
- Gradient Accumulation
- Mixed Precision (FP16/BF16)
- Initialization Strategy (Xavier/He/Orthogonal)

Debugging Techniques:
- Overfit Single Batch Verification
- Learning Rate Range Test
- Gradient Distribution Monitoring
- Activation Value Visualization
```

### Model Evaluation Deep Dive
```
Offline Evaluation:
- Stratified Sampling Evaluation
- Time Split Evaluation
- Cross-Validation Strategy
- Statistical Significance Testing

Online Evaluation:
- A/B Test Design
- Multi-Armed Bandit
- Cross-Effect Validation
- Long-Term Effect Tracking

Business Metrics Alignment:
- Model Metrics â†’ Business Metrics Mapping
- Cost-Sensitive Evaluation
- Profit Curve Analysis
- Threshold Optimization
```

### Inference Optimization Deep Dive
```
Model Compression:
- Quantization: PTQ/QAT
- Pruning: Structured/Unstructured
- Distillation: Teacher-Student
- NAS: Automatic Architecture Search

Runtime Optimization:
- Operator Fusion
- Memory Optimization
- Batching Strategy
- Caching Strategy

Hardware Adaptation:
- TensorRT (NVIDIA)
- ONNX Runtime
- OpenVINO (Intel)
- CoreML/Metal (Apple)
```

### Production-Grade Monitoring
```
Data Monitoring:
- Feature Distribution Drift (KS/PSI)
- Missing Value Monitoring
- Data Latency Monitoring
- Data Quality Score

Model Monitoring:
- Prediction Distribution Change
- Confidence Distribution
- Feature Importance Change
- Performance Metrics Trend

Alert Strategy:
- Tiered Alerting (P0-P3)
- Automatic Retraining Trigger
- Manual Review Trigger
- Rollback Mechanism
```

---

## ğŸš¨ Common Pitfalls for Senior ML Experts

### Data Pitfalls
```
âŒ Data Leakage:
- Future information enters training
- Test set contamination
- Correct approach: Strict time split, feature review

âŒ Sampling Bias:
- Samples do not represent true distribution
- Improper class imbalance handling
- Correct approach: Stratified sampling, weighted loss

âŒ Feature Inconsistency:
- Different training and inference feature computation
- Feature version mismatch
- Correct approach: Feature store, unified pipeline
```

### Model Pitfalls
```
âŒ Over-Complexity:
- Use complex models from the start
- No baseline comparison
- Correct approach: Simple baseline first

âŒ Metric Optimization Excess:
- Focus only on single metric
- Ignore business impact
- Correct approach: Multi-metric balance, business alignment

âŒ Improper Validation:
- Random split time series
- No consideration of data distribution
- Correct approach: Design validation based on business scenario
```

### Production Pitfalls
```
âŒ Model Aging:
- No monitoring after deployment
- Data drift not handled
- Correct approach: Continuous monitoring, periodic retraining

âŒ Ignoring Inference Performance:
- Focus only on model accuracy
- Latency cannot meet SLA
- Correct approach: Balance performance and accuracy

âŒ Missing Rollback Mechanism:
- New model online has issues
- Cannot quickly rollback
- Correct approach: Blue-green deployment, fast rollback
```

---

## ğŸ“Š Performance Monitoring Metrics

| Metric | Target | Alert Threshold | Measurement Tool |
|--------|--------|-----------------|------------------|
| Model Accuracy | Business Based | Down 5% | Evaluation Pipeline |
| Inference Latency (P99) | < 100ms | > 500ms | APM |
| Feature Drift (PSI) | < 0.1 | > 0.25 | Monitoring System |
| Prediction Distribution Shift | < 0.1 | > 0.2 | Monitoring System |
| Model Training Time | Scenario Based | > 2x Baseline | MLflow |
| GPU Utilization | > 80% | < 50% | Hardware Monitoring |
| Feature Freshness | < 1 hour | > 24 hours | Feature Store |
| Data Quality Score | > 99% | < 95% | Quality Platform |
| Experiment Success Rate | Team Based | Abnormal Drop | Experiment Platform |
| Model Update Frequency | Business Based | Exceed Threshold | Deployment System |

---

## ğŸ“‹ Machine Learning Development Checklist (Complete)

### Data Pipeline
- [ ] Data quality validation
- [ ] Proper train/validation/test split
- [ ] No data leakage
- [ ] Feature engineering versioning

### Model Development
- [ ] Baseline model establishment
- [ ] Hyperparameter tuning (validation set)
- [ ] Multi-metric evaluation
- [ ] Reproducibility guarantee

### Production Deployment
- [ ] Inference performance optimization
- [ ] Model version management
- [ ] Blue-green/Canary deployment
- [ ] Rollback mechanism

### Continuous Operations
- [ ] Data drift monitoring
- [ ] Model performance monitoring
- [ ] Automatic retraining trigger
- [ ] Alerts and On-call

---

**Machine Learning Development Principles Summary**:
Data Quality, Feature Engineering, Model Evaluation, Reproducibility, Experiment Tracking, Deployment Optimization, Monitoring Drift, Security and Privacy, Fairness, Continuous Improvement
