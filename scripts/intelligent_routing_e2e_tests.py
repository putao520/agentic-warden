#!/usr/bin/env python3
"""
Agentic-Warden æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ E2E æµ‹è¯•
åŸºäºçœŸå® OLLAMA å’Œ CODEX çš„å®Œæ•´æµ‹è¯•
"""

import os
import sys
import json
import time
import subprocess
import shutil
import tempfile
import requests
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import re

class TestStatus(Enum):
    PASSED = "PASSED"
    FAILED = "FAILED"
    SKIPPED = "SKIPPED"
    ERROR = "ERROR"

@dataclass
class TestResult:
    name: str
    status: TestStatus
    duration: float
    output: str
    error: Optional[str] = None
    route_type: Optional[str] = None  # 'js_workflow' or 'direct_mcp'
    quality_score: Optional[Dict[str, float]] = None

@dataclass
class QualityScore:
    syntax_correctness: float = 0.0
    logic_correctness: float = 0.0
    security_score: float = 0.0
    efficiency_score: float = 0.0
    maintainability: float = 0.0
    overall_score: float = 0.0

class IntelligentRoutingTester:
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.aiw_binary = project_root / "target/release/aiw"
        self.test_results_dir = project_root / "test-results"
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.report_file = self.test_results_dir / f"routing_e2e_report_{self.timestamp}.md"

        # ç»Ÿè®¡
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.skipped_tests = 0

        # æµ‹è¯•ç»“æœ
        self.test_results: List[TestResult] = []

        # åˆ›å»ºç»“æœç›®å½•
        self.test_results_dir.mkdir(exist_ok=True)

        # é…ç½®MCPæœåŠ¡å™¨
        self.setup_mcp_config()

    def log_info(self, message: str):
        print(f"[INFO] {message}")
        self._write_to_report(f"**{message}**")

    def log_success(self, message: str):
        print(f"[PASS] {message}")
        self._write_to_report(f"âœ… {message}")
        self.passed_tests += 1

    def log_error(self, message: str):
        print(f"[FAIL] {message}")
        self._write_to_report(f"âŒ {message}")
        self.failed_tests += 1

    def log_warning(self, message: str):
        print(f"[WARN] {message}")
        self._write_to_report(f"âš ï¸ {message}")

    def log_test(self, message: str):
        print(f"[TEST] {message}")
        self._write_to_report(f"ğŸ§ª {message}")
        self.total_tests += 1

    def _write_to_report(self, content: str):
        try:
            with open(self.report_file, 'a', encoding='utf-8') as f:
                f.write(f"{content}\n")
        except IOError as e:
            print(f"Warning: Could not write to report file: {e}")

    def setup_mcp_config(self):
        """è®¾ç½®MCPé…ç½®"""
        mcp_config = {
            "mcpServers": {
                "filesystem": {
                    "command": "npx",
                    "args": ["@modelcontextprotocol/server-filesystem", "/tmp"],
                    "enabled": True
                },
                "memory": {
                    "command": "npx",
                    "args": ["@modelcontextprotocol/server-memory"],
                    "enabled": True
                }
            }
        }

        config_path = Path.home() / ".aiw" / "mcp.json"
        config_path.parent.mkdir(exist_ok=True)

        with open(config_path, 'w') as f:
            json.dump(mcp_config, f, indent=2)

        self.log_info(f"å·²é…ç½®MCPæœåŠ¡å™¨: {config_path}")

    def run_command(self, command: str, timeout: int = 60, input_data: Optional[str] = None) -> Tuple[int, str, str]:
        """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout,
                input=input_data,
                cwd=self.project_root
            )
            return result.returncode, result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            return -1, "", "Command timed out"

    def check_ollama_available(self) -> bool:
        """æ£€æŸ¥OLLAMAæ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            return result.returncode == 0
        except FileNotFoundError:
            return False

    def check_ollama_model(self, model: str) -> bool:
        """æ£€æŸ¥OLLAMAæ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            return model in result.stdout
        except (FileNotFoundError, subprocess.SubprocessError):
            return False

    def evaluate_code_quality(self, code: str) -> QualityScore:
        """è¯„ä¼°ä»£ç ç”Ÿæˆè´¨é‡"""
        score = QualityScore()

        # è¯­æ³•æ­£ç¡®æ€§æ£€æŸ¥
        try:
            compile(code, '<string>', 'exec')
            score.syntax_correctness = 1.0
        except SyntaxError:
            score.syntax_correctness = 0.0
        except Exception:
            score.syntax_correctness = 0.5  # å…¶ä»–é”™è¯¯å¯èƒ½æ˜¯è¿è¡Œæ—¶é—®é¢˜

        # å®‰å…¨æ€§æ£€æŸ¥
        security_patterns = [
            r'eval\s*\(',  # evalä½¿ç”¨
            r'exec\s*\(',  # execä½¿ç”¨
            r'subprocess\.',  # subprocessæœªè¿‡æ»¤
            r'os\.system',  # os.systemä½¿ç”¨
            r'shell=True',  # shell=Trueé£é™©
        ]

        security_issues = sum(1 for pattern in security_patterns if re.search(pattern, code, re.IGNORECASE))
        score.security_score = max(0.0, 1.0 - (security_issues * 0.2))

        # å¯ç»´æŠ¤æ€§æ£€æŸ¥
        maintainability_patterns = [
            r'def\s+\w+\s*\([^)]*\):',  # å‡½æ•°å®šä¹‰
            r'class\s+\w+',  # ç±»å®šä¹‰
            r'#.*',  # æ³¨é‡Š
            r'""".*"""',  # æ–‡æ¡£å­—ç¬¦ä¸²
            r"'.*'",  # æ–‡æ¡£å­—ç¬¦ä¸²
        ]

        maintainability_score = sum(1 for pattern in maintainability_patterns if re.search(pattern, code, re.MULTILINE | re.DOTALL))
        score.maintainability = min(1.0, maintainability_score / 3.0)  # æœ€å¤š3åˆ†

        # æ•ˆç‡æ£€æŸ¥ï¼ˆç®€å•å¯å‘å¼ï¼‰
        efficiency_indicators = [
            r'async\s+def',  # å¼‚æ­¥å‡½æ•°
            r'await\s+',  # å¼‚æ­¥ç­‰å¾…
            r'for.*in.*range',  # å¾ªç¯æ•ˆç‡
            r'list\s*comprehension',  # åˆ—è¡¨æ¨å¯¼
        ]

        efficiency_score = sum(1 for pattern in efficiency_indicators if re.search(pattern, code, re.MULTILINE))
        score.efficiency = min(1.0, efficiency_score / 2.0)  # æœ€å¤š2åˆ†

        # é€»è¾‘æ­£ç¡®æ€§ï¼ˆåŸºäºä»£ç ç»“æ„å’Œå¤æ‚åº¦çš„å¯å‘å¼è¯„ä¼°ï¼‰
        code_lines = len([line for line in code.split('\n') if line.strip()])
        logic_complexity = min(1.0, code_lines / 50.0)  # åŸºäºè¡Œæ•°çš„ç®€å•è¯„ä¼°
        score.logic_correctness = 0.7 + 0.3 * logic_complexity  # åŸºç¡€åˆ†+å¤æ‚åº¦åˆ†

        # è®¡ç®—æ€»åˆ†
        score.overall_score = (
            score.syntax_correctness * 0.3 +
            score.logic_correctness * 0.2 +
            score.security_score * 0.2 +
            score.efficiency_score * 0.15 +
            score.maintainability * 0.15
        )

        return score

    def test_intelligent_routing_with_request(self, user_request: str, expected_route: str = None) -> TestResult:
        """æµ‹è¯•æ™ºèƒ½è·¯ç”±åŠŸèƒ½"""
        self.log_test(f"æ™ºèƒ½è·¯ç”±æµ‹è¯•: {user_request[:50]}...")

        start_time = time.time()

        # æ„é€ MCPå·¥å…·è°ƒç”¨è¯·æ±‚
        mcp_request = {
            "jsonrpc": "2.0",
            "method": "tools/call",
            "params": {
                "name": "intelligent_route",
                "arguments": {
                    "user_request": user_request
                }
            },
            "id": 1
        }

        # è°ƒç”¨æ™ºèƒ½è·¯ç”±
        code, stdout, stderr = self.run_command(
            f"echo '{json.dumps(mcp_request)}' | timeout 30s {self.aiw_binary} mcp serve",
            timeout=45
        )

        duration = time.time() - start_time

        # åˆ†æå“åº”
        route_type = None
        if "JavaScript" in stdout or "workflow" in stdout:
            route_type = "js_workflow"
        elif "direct" in stdout or "MCP" in stdout:
            route_type = "direct_mcp"

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
        if code == 0 or (code == -1 and "timeout" not in stderr.lower()):
            # ä»£ç è´¨é‡è¯„ä¼°ï¼ˆå¦‚æœç”Ÿæˆä»£ç ï¼‰
            quality_score = None
            if "function" in stdout or "async" in stdout:
                # æå–ç”Ÿæˆçš„ä»£ç ç‰‡æ®µ
                code_pattern = r'```(?:javascript|js)\n(.*?)\n```'
                matches = re.findall(code_pattern, stdout, re.DOTALL | re.MULTILINE)
                if matches:
                    quality_score = asdict(self.evaluate_code_quality(matches[0]))

            result = TestResult(
                name=f"è·¯ç”±æµ‹è¯•: {user_request[:30]}...",
                status=TestStatus.PASSED,
                duration=duration,
                output=stdout[:1000],
                route_type=route_type,
                quality_score=quality_score
            )

            self.log_success(f"è·¯ç”±æµ‹è¯•å®Œæˆ: {route_type}")
            return result
        else:
            error_msg = stderr[:500] if stderr else "Unknown error"
            result = TestResult(
                name=f"è·¯ç”±æµ‹è¯•: {user_request[:30]}...",
                status=TestStatus.FAILED,
                duration=duration,
                output=stdout[:500],
                error=error_msg,
                route_type=route_type
            )

            self.log_error(f"è·¯ç”±æµ‹è¯•å¤±è´¥: {error_msg}")
            return result

    def test_ollama_integration(self) -> List[TestResult]:
        """æµ‹è¯•OLLAMAé›†æˆ"""
        if not self.check_ollama_available():
            self.log_warning("OLLAMAä¸å¯ç”¨ï¼Œè·³è¿‡OLLAMAæµ‹è¯•")
            return [TestResult(
                name="OLLAMAé›†æˆæµ‹è¯•",
                status=TestStatus.SKIPPED,
                duration=0,
                output="OLLAMAä¸å¯ç”¨",
                error="OLLAMAæœªå®‰è£…æˆ–æœªè¿è¡Œ"
            )]

        results = []

        # æµ‹è¯•ä¸åŒæ¨¡å‹
        models_to_test = ["llama3.1:8b"]
        if self.check_ollama_model("llama3.1:70b"):
            models_to_test.append("llama3.1:70b")

        test_requests = [
            "ç”Ÿæˆä¸€ä¸ªPythonå‡½æ•°ï¼Œè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
            "åˆ›å»ºä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨å¤„ç†GETè¯·æ±‚",
            "å®ç°æ–‡ä»¶è¯»å–å’Œæ•°æ®è§£æåŠŸèƒ½"
        ]

        for model in models_to_test:
            self.log_info(f"æµ‹è¯•OLLAMAæ¨¡å‹: {model}")

            for request in test_requests:
                # æ„é€ åŒ…å«OLLAMAçš„è¯·æ±‚
                mcp_request = {
                    "jsonrpc": "2.0",
                    "method": "tools/call",
                    "params": {
                        "name": "intelligent_route_with_ollama",
                        "arguments": {
                            "user_request": request,
                            "model": model
                        }
                    },
                    "id": 1
                }

                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…APIè°ƒæ•´
                # å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç‰¹å®šçš„OLLAMAå·¥å…·
                start_time = time.time()
                code, stdout, stderr = self.run_command(
                    f"echo '{json.dumps(mcp_request)}' | timeout 60s {self.aiw_binary} mcp serve",
                    timeout=90
                )
                duration = time.time() - start_time

                if code == 0 or "ollama" in stdout.lower():
                    quality_score = None
                    if "def " in stdout or "function" in stdout:
                        # è¯„ä¼°ç”Ÿæˆçš„ä»£ç è´¨é‡
                        quality_score = asdict(self.evaluate_code_quality(stdout))

                    result = TestResult(
                        name=f"OLLAMA {model}: {request[:30]}...",
                        status=TestStatus.PASSED,
                        duration=duration,
                        output=stdout[:1000],
                        route_type="ollama_generated",
                        quality_score=quality_score
                    )

                    self.log_success(f"OLLAMA {model} æµ‹è¯•é€šè¿‡")
                else:
                    result = TestResult(
                        name=f"OLLAMA {model}: {request[:30]}...",
                        status=TestStatus.FAILED,
                        duration=duration,
                        output=stdout[:500],
                        error=stderr[:500],
                        route_type="ollama_failed"
                    )

                    self.log_error(f"OLLAMA {model} æµ‹è¯•å¤±è´¥")

                results.append(result)

        return results

    def test_real_codex_integration(self) -> List[TestResult]:
        """æµ‹è¯•çœŸå®CODEXé›†æˆ"""
        if not shutil.which("codex"):
            self.log_warning("CODEXä¸å¯ç”¨ï¼Œè·³è¿‡CODEXæµ‹è¯•")
            return [TestResult(
                name="CODEXé›†æˆæµ‹è¯•",
                status=TestStatus.SKIPPED,
                duration=0,
                output="CODEXä¸å¯ç”¨",
                error="CODEXæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
            )]

        results = []

        test_requests = [
            "åˆ›å»ºä¸€ä¸ªREST APIç«¯ç‚¹å¤„ç†ç”¨æˆ·è®¤è¯",
            "å®ç°ä¸€ä¸ªReactç»„ä»¶æ”¯æŒè¡¨æ ¼æ’åºå’Œåˆ†é¡µ",
            "ç”Ÿæˆæ•°æ®ç¼“å­˜æœºåˆ¶æ”¯æŒTTL",
            "åˆ›å»ºä¸€ä¸ªWebSocketæœåŠ¡å™¨å¤„ç†å®æ—¶é€šä¿¡"
        ]

        self.log_info("æµ‹è¯•çœŸå®CODEXé›†æˆ")

        for request in test_requests:
            start_time = time.time()

            # ä½¿ç”¨çœŸå®CODEXç¯å¢ƒå˜é‡
            codex_bin = shutil.which("codex")
            env = os.environ.copy()
            env["CODEX_BIN"] = codex_bin

            code, stdout, stderr = self.run_command(
                f"CODEX_BIN={codex_bin} ./target/debug/test_launch",
                timeout=60
            )

            duration = time.time() - start_time

            if code == 0 and "Task launched successfully" in stdout:
                # åˆ†æç”Ÿæˆä»£ç çš„è´¨é‡
                quality_score = None

                # æå–ä»»åŠ¡è¾“å‡º
                if "PID:" in stdout:
                    # å°è¯•è·å–ç”Ÿæˆçš„ä»£ç 
                    pid_match = re.search(r'PID:\s*(\d+)', stdout)
                    if pid_match:
                        pid = pid_match.group(1)
                        # è¿™é‡Œå¯ä»¥è¯»å–ç”Ÿæˆçš„ä»£ç æ–‡ä»¶æˆ–æ—¥å¿—æ¥è¯„ä¼°è´¨é‡

                result = TestResult(
                    name=f"CODEX: {request[:30]}...",
                    status=TestStatus.PASSED,
                    duration=duration,
                    output=stdout[:1000],
                    route_type="codex_real",
                    quality_score=quality_score
                )

                self.log_success(f"CODEXæµ‹è¯•é€šè¿‡: {request[:30]}...")
            else:
                result = TestResult(
                    name=f"CODEX: {request[:30]}...",
                    status=TestStatus.FAILED,
                    duration=duration,
                    output=stdout[:500],
                    error=stderr[:500],
                    route_type="codex_failed"
                )

                self.log_error(f"CODEXæµ‹è¯•å¤±è´¥: {request[:30]}...")

            results.append(result)

        return results

    def test_routing_decision_accuracy(self) -> List[TestResult]:
        """æµ‹è¯•è·¯ç”±å†³ç­–å‡†ç¡®æ€§"""
        results = []

        # ç®€å•æ“ä½œåº”è¯¥èµ°ç›´æ¥MCPè·¯ç”±
        simple_operations = [
            ("è¯»å–æ–‡ä»¶å†…å®¹", "direct_mcp"),
            ("åˆ—å‡ºç›®å½•å†…å®¹", "direct_mcp"),
            ("å†™å…¥æ•°æ®åˆ°å†…å­˜", "direct_mcp")
        ]

        # å¤æ‚æ“ä½œåº”è¯¥èµ°JavaScriptå·¥ä½œæµè·¯ç”±
        complex_operations = [
            ("è¯»å–JSONæ–‡ä»¶ï¼Œå¤„ç†æ•°æ®åå†™å…¥æ–°æ–‡ä»¶", "js_workflow"),
            ("ç›‘æ§æ–‡ä»¶å˜åŒ–å¹¶è®°å½•åˆ°å†…å­˜ç³»ç»Ÿ", "js_workflow"),
            ("è§£æé…ç½®æ–‡ä»¶å¹¶ç”ŸæˆæŠ¥å‘Š", "js_workflow")
        ]

        self.log_info("æµ‹è¯•è·¯ç”±å†³ç­–å‡†ç¡®æ€§")

        for request, expected_route in simple_operations + complex_operations:
            result = self.test_intelligent_routing_with_request(request, expected_route)

            # éªŒè¯è·¯ç”±æ˜¯å¦æ­£ç¡®
            if result.route_type == expected_route:
                result.status = TestStatus.PASSED
                self.log_success(f"è·¯ç”±å†³ç­–æ­£ç¡®: {request[:30]} -> {expected_route}")
            else:
                result.status = TestStatus.FAILED
                result.error = f"Expected {expected_route}, got {result.route_type}"
                self.log_error(f"è·¯ç”±å†³ç­–é”™è¯¯: {request[:30]} -> {result.route_type}")

            results.append(result)

        return results

    def init_report(self):
        """åˆå§‹åŒ–æµ‹è¯•æŠ¥å‘Š"""
        # æ£€æŸ¥ç¯å¢ƒ
        ollama_available = self.check_ollama_available()
        codex_available = shutil.which("codex") is not None

        available_models = []
        if ollama_available:
            for model in ["llama3.1:8b", "llama3.1:70b"]:
                if self.check_ollama_model(model):
                    available_models.append(model)

        with open(self.report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# Agentic-Warden æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ E2E æµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**é¡¹ç›®æ ¹ç›®å½•**: {self.project_root}
**AIWäºŒè¿›åˆ¶**: {self.aiw_binary}
**Pythonç‰ˆæœ¬**: {sys.version}

## ç¯å¢ƒæ£€æŸ¥

- **OLLAMAå¯ç”¨**: {'âœ…' if ollama_available else 'âŒ'}
- **CODEXå¯ç”¨**: {'âœ…' if codex_available else 'âŒ'}
- **å¯ç”¨OLLAMAæ¨¡å‹**: {', '.join(available_models) if available_models else 'æ— '}

## æµ‹è¯•æ¦‚è§ˆ

""")

    def finalize_report(self):
        """å®Œæˆæµ‹è¯•æŠ¥å‘Š"""
        success_rate = (self.passed_tests * 100 // self.total_tests) if self.total_tests > 0 else 0

        with open(self.report_file, 'a', encoding='utf-8') as f:
            f.write(f"""
## æµ‹è¯•æ€»ç»“

- **æ€»æµ‹è¯•æ•°**: {self.total_tests}
- **é€šè¿‡æµ‹è¯•**: {self.passed_tests}
- **å¤±è´¥æµ‹è¯•**: {self.failed_tests}
- **è·³è¿‡æµ‹è¯•**: {self.skipped_tests}
- **æˆåŠŸç‡**: {success_rate}%

## è¯¦ç»†æµ‹è¯•ç»“æœ

""")

            # æŒ‰è·¯ç”±ç±»å‹åˆ†ç»„ç»“æœ
            route_groups = {}
            for result in self.test_results:
                route_type = result.route_type or "unknown"
                if route_type not in route_groups:
                    route_groups[route_type] = []
                route_groups[route_type].append(result)

            for route_type, results in route_groups.items():
                f.write(f"### {route_type.replace('_', ' ').title()}\n\n")

                for result in results:
                    status_icon = "âœ…" if result.status == TestStatus.PASSED else "âŒ"
                    f.write(f"- {status_icon} **{result.name}** ({result.status.value})\n")
                    f.write(f"  - è€—æ—¶: {result.duration:.2f}s\n")
                    if result.error:
                        f.write(f"  - é”™è¯¯: {result.error[:200]}\n")
                    if result.quality_score:
                        f.write(f"  - è´¨é‡è¯„åˆ†: {result.quality_score.get('overall_score', 0):.2f}/1.0\n")
                f.write("\n")

            # è´¨é‡ç»Ÿè®¡
            quality_scores = [r.quality_score for r in self.test_results if r.quality_score]
            if quality_scores:
                avg_syntax = sum(s.get('syntax_correctness', 0) for s in quality_scores) / len(quality_scores)
                avg_security = sum(s.get('security_score', 0) for s in quality_scores) / len(quality_scores)
                avg_overall = sum(s.get('overall_score', 0) for s in quality_scores) / len(quality_scores)

                f.write(f"""### ä»£ç ç”Ÿæˆè´¨é‡ç»Ÿè®¡

- **å¹³å‡è¯­æ³•æ­£ç¡®æ€§**: {avg_syntax:.2f}/1.0
- **å¹³å‡å®‰å…¨æ€§è¯„åˆ†**: {avg_security:.2f}/1.0
- **å¹³å‡æ€»ä½“è´¨é‡**: {avg_overall:.2f}/1.0

""")

        if self.failed_tests == 0:
            print(f"\nğŸ‰ æ‰€æœ‰æ™ºèƒ½è·¯ç”±æµ‹è¯•é€šè¿‡ï¼æˆåŠŸç‡: {success_rate}%")
        else:
            print(f"\nâš ï¸  æœ‰ {self.failed_tests} ä¸ªæµ‹è¯•å¤±è´¥ã€‚æˆåŠŸç‡: {success_rate}%")

        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: {self.report_file}")

    def run_all_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰æ™ºèƒ½è·¯ç”±E2Eæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹Agentic-Warden æ™ºèƒ½è·¯ç”±ç³»ç»ŸE2Eæµ‹è¯•")
        print("=" * 60)

        # åˆå§‹åŒ–æŠ¥å‘Š
        self.init_report()

        # æ£€æŸ¥æ„å»º
        if not self.aiw_binary.exists():
            self.log_info("æ„å»ºAgentic-Warden...")
            subprocess.run("cargo build --release", shell=True, cwd=self.project_root)

        # è¿è¡Œæµ‹è¯•å¥—ä»¶
        try:
            # 1. è·¯ç”±å†³ç­–å‡†ç¡®æ€§æµ‹è¯•
            self.log_info("=== è·¯ç”±å†³ç­–å‡†ç¡®æ€§æµ‹è¯• ===")
            routing_results = self.test_routing_decision_accuracy()
            self.test_results.extend(routing_results)

            # 2. OLLAMAé›†æˆæµ‹è¯•
            self.log_info("=== OLLAMAé›†æˆæµ‹è¯• ===")
            ollama_results = self.test_ollama_integration()
            self.test_results.extend(ollama_results)

            # 3. çœŸå®CODEXé›†æˆæµ‹è¯•
            self.log_info("=== çœŸå®CODEXé›†æˆæµ‹è¯• ===")
            codex_results = self.test_real_codex_integration()
            self.test_results.extend(codex_results)

        except Exception as e:
            self.log_error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
            return False

        # ç”ŸæˆæŠ¥å‘Š
        self.finalize_report()

        # è¿”å›æ˜¯å¦å…¨éƒ¨é€šè¿‡
        return self.failed_tests == 0

def main():
    """ä¸»å‡½æ•°"""
    project_root = Path(__file__).parent.parent
    tester = IntelligentRoutingTester(project_root)

    success = tester.run_all_tests()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()